import awswrangler as wr
import pandas as pd
import numpy as np


# Load the CLEANED dataset we prepared in the previous phase
cleaned_s3_path = "s3://chung-yeh-youbike-poc-data/processed/youbike_data_for_modeling.parquet"
print(f"Loading dataset from {cleaned_s3_path}...")
df = wr.s3.read_parquet(path=cleaned_s3_path)
print("Dataset loaded successfully!")

df.reset_index(inplace=True)








# Create Lag & Rolling Window Features
features_to_engineer = ['available_rent_bikes', 'available_return_bikes']
lag_steps = [1, 2, 3, 6] 
rolling_window_size = 6 

for col in features_to_engineer:
    print(f"Engineering advanced features for column: {col}")
    for step in lag_steps:
        df[f'{col}_lag_{step}'] = df.groupby('sno')[col].shift(step)
    
   
    rolling_window = df.groupby('sno')[col].rolling(window=rolling_window_size)
    df[f'{col}_rolling_mean'] = rolling_window.mean().values
    df[f'{col}_rolling_std'] = rolling_window.std().values
    df[f'{col}_rolling_min'] = rolling_window.min().values
    df[f'{col}_rolling_max'] = rolling_window.max().values

# Drop rows with NaN values created by the shifting and rolling operations
df.dropna(inplace=True)

print("\nFeature engineering complete.")
print("New DataFrame shape:", df.shape)








import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt





features = [
    'total', 'available_rent_bikes', 'latitude', 'longitude', 'available_return_bikes',
    'hour', 'day_of_week', 'is_weekend',
    'available_rent_bikes_lag_1', 'available_rent_bikes_lag_2', 'available_rent_bikes_lag_3', 'available_rent_bikes_lag_6',
    'available_rent_bikes_rolling_mean', 'available_rent_bikes_rolling_std', 'available_rent_bikes_rolling_min', 'available_rent_bikes_rolling_max',
    'available_return_bikes_lag_1', 'available_return_bikes_lag_2', 'available_return_bikes_lag_3', 'available_return_bikes_lag_6',
    'available_return_bikes_rolling_mean', 'available_return_bikes_rolling_std', 'available_return_bikes_rolling_min', 'available_return_bikes_rolling_max'
]
target = 'status_in_15_mins'

X = df[features]
y = df[target]

# Convert categorical target to numerical codes
y = y.astype('category')
label_map = dict(enumerate(y.cat.categories))
y_codes = y.cat.codes





X_train, X_test, y_train, y_test = train_test_split(
    X, y_codes, test_size=0.2, shuffle=False
)





print("\nTraining Improved LightGBM model...")
# adding class_weight='balanced' to improve the precision of minority class
lgb_clf_improved = lgb.LGBMClassifier(random_state=42, class_weight='balanced')
lgb_clf_improved.fit(X_train, y_train)
print("Model training complete.")





y_pred = lgb_clf_improved.predict(X_test)

# Convert numerical predictions back to original labels
y_test_labels = [label_map[i] for i in y_test]
y_pred_labels = [label_map[i] for i in y_pred]

# Print the classification report
print("\n--- Improved Model Classification Report ---")
print(classification_report(y_test_labels, y_pred_labels))

# Display the confusion matrix
print("\n--- Improved Model Confusion Matrix ---")
cm = confusion_matrix(y_test_labels, y_pred_labels, labels=list(label_map.values()))
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=list(label_map.values()), yticklabels=list(label_map.values()))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Improved Model Confusion Matrix')
plt.show()





import joblib
import awswrangler as wr

# --- Save the Model Locally First ---
model_filename = 'youbike_lgbm_model.joblib'
joblib.dump(lgb_clf_improved, model_filename)
print(f"Model saved locally as {model_filename}")

# --- Upload the Model to S3 ---
model_s3_path = "s3://chung-yeh-youbike-poc-data/models/youbike_lgbm_model.joblib"
print(f"Uploading model to {model_s3_path}...")

# Use awswrangler to upload the file
wr.s3.upload(local_file=model_filename, path=model_s3_path)

print("Model successfully uploaded to S3!")
